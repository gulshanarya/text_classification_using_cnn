{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiN-RGTxun3O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, Concatenate, MaxPooling1D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ78NaljsO8t",
        "outputId": "0ced5183-9193-45e3-d3b4-36568b158560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA4H3zfMsbYT",
        "outputId": "1f8ca033-1a31-43eb-eaf3-53e68ea1bac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/text_classification_with_CNN/'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/text_classification_with_CNN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bg-X4SGufsq",
        "outputId": "4b67dad6-fefa-4d6c-9f17-a465425102e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'preprocessed_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-137d978aeb1f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessed_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocessed_data.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thb0Y48uusb5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRra9kyyAKSH"
      },
      "outputs": [],
      "source": [
        "#Source: andrew ng course: deeplearning specialization\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfIm6dJbuxwB"
      },
      "outputs": [],
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am-ngtabpI0S"
      },
      "source": [
        "#Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syNsyLyMu6Cj"
      },
      "outputs": [],
      "source": [
        "# prompt: print all the text having nan\n",
        "\n",
        "df[df.isna().any(axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0TC721vvgKR"
      },
      "outputs": [],
      "source": [
        "# prompt: drop all the rows having any column nan values\n",
        "\n",
        "df.dropna(axis=0, how='any', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm5wqtrIv0B_"
      },
      "outputs": [],
      "source": [
        "# prompt: check nan value\n",
        "\n",
        "df[df.isna().any(axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvHqAZ95A58B"
      },
      "outputs": [],
      "source": [
        "texts = df[\"text_combined\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iBdu7xfeuXP"
      },
      "outputs": [],
      "source": [
        "# prompt: wants to know the distribution of the length of words in each texts\n",
        "\n",
        "len_words = []\n",
        "for text in texts:\n",
        "    words = text.split()\n",
        "    len_words.append(len(words))\n",
        "\n",
        "# Plotting the histogram\n",
        "# Define the bin width\n",
        "bin_width = 500\n",
        "\n",
        "# Plotting the histogram with specified bin width\n",
        "bins = np.arange(min(len_words), max(len_words) + bin_width, bin_width)\n",
        "plt.hist(len_words, bins=bins, edgecolor='black')\n",
        "plt.title('Distribution of Numbers')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2WsgRXczm1N"
      },
      "outputs": [],
      "source": [
        "label = df[\"Label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKax11hts3ZP"
      },
      "outputs": [],
      "source": [
        "# prompt: convert word_to_index to dictionary\n",
        "word_to_index_dict = dict(word_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrxKTwqj2wzX"
      },
      "outputs": [],
      "source": [
        "len(word_to_index_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDLBWPcyrkoL"
      },
      "outputs": [],
      "source": [
        "train_data = list()\n",
        "\n",
        "for sent in tqdm(texts):\n",
        "  d = list()\n",
        "  for word in sent:\n",
        "\n",
        "    if word in word_to_index_dict:\n",
        "      d.append(word_to_index_dict[word])\n",
        "  train_data.append(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap9BtF-PtuJv"
      },
      "outputs": [],
      "source": [
        "# prompt: pad train_data with 0 and make the equal length\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = 500\n",
        "train_data = pad_sequences(train_data, maxlen=max_length, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGDsePuDDtMX",
        "outputId": "d625b081-3045-4bbe-e1dd-a62382c8092b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15963, 500)\n",
            "(3991, 500)\n",
            "(15963,)\n",
            "(3991,)\n"
          ]
        }
      ],
      "source": [
        "import sklearn.model_selection\n",
        "\n",
        "# Split the data into train and test sets.\n",
        "texts_train, texts_test, labels_train, labels_test = sklearn.model_selection.train_test_split(train_data, label, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the train and test sets.\n",
        "print(texts_train.shape)\n",
        "print(texts_test.shape)\n",
        "print(labels_train.shape)\n",
        "print(labels_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EEIJfl4snb-"
      },
      "outputs": [],
      "source": [
        "#free variable\n",
        "del train_data, texts, df, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsKllh-X_ChH"
      },
      "source": [
        "**Experiments:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP1vuLQqCjT-"
      },
      "outputs": [],
      "source": [
        "NUM_FILTERS=16\n",
        "EMBEDDING_DIM = 50\n",
        "M=3\n",
        "N=3\n",
        "O=3\n",
        "P=5\n",
        "Q=5\n",
        "R=5\n",
        "S=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYL2vcfV3Nor",
        "outputId": "a8932fb6-b11f-4573-d79f-b8e53b830215"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(word_to_index_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ELL536-_OY-"
      },
      "outputs": [],
      "source": [
        "#Source chatgpt with detailed prompt, giving the complete information of the architecture one by one.\n",
        "\n",
        "# Define the input shape\n",
        "input_layer = tf.keras.layers.Input(shape=(500,), name='InputLayer')\n",
        "# reshaped_input = tf.expand_dims(input_layer, axis=-1)\n",
        "\n",
        "word_embedding_layer = tf.keras.layers.Embedding(input_dim=400000, output_dim=50, name='CharEmbedding')(input_layer)\n",
        "transposed_input = tf.transpose(word_embedding_layer, perm=[0, 2, 1])  # Assuming you want to transpose the input\n",
        "\n",
        "# Convolutional layers with different filter sizes\n",
        "conv1 = Conv1D(filters=NUM_FILTERS, kernel_size=M, activation='relu')(transposed_input)\n",
        "conv2 = Conv1D(filters=NUM_FILTERS, kernel_size=N, activation='relu')(transposed_input)\n",
        "conv3 = Conv1D(filters=NUM_FILTERS, kernel_size=O, activation='relu')(transposed_input)\n",
        "\n",
        "# Concatenate the convolutional layers\n",
        "concatenated = Concatenate(axis=1)([conv1, conv2, conv3])\n",
        "\n",
        "# Max pooling layer\n",
        "maxpool1 = MaxPooling1D(pool_size=2)(concatenated)\n",
        "\n",
        "# Convolutional layers with different filter sizes\n",
        "conv4 = Conv1D(filters=NUM_FILTERS, kernel_size=P, activation='relu')(maxpool1)\n",
        "conv5 = Conv1D(filters=NUM_FILTERS, kernel_size=Q, activation='relu')(maxpool1)\n",
        "conv6 = Conv1D(filters=NUM_FILTERS, kernel_size=R, activation='relu')(maxpool1)\n",
        "\n",
        "# Concatenate the convolutional layers\n",
        "concatenated2 = Concatenate(axis=1)([conv4, conv5, conv6])\n",
        "\n",
        "# Max pooling layer\n",
        "maxpool2 = MaxPooling1D(pool_size=2)(concatenated2)\n",
        "\n",
        "# Convolutional layer\n",
        "conv7 = Conv1D(filters=NUM_FILTERS, kernel_size=S, activation='relu')(maxpool2)\n",
        "\n",
        "# Flatten layer\n",
        "flatten = Flatten()(conv7)\n",
        "\n",
        "# Dropout layer\n",
        "dropout = Dropout(rate=0.5)(flatten)\n",
        "\n",
        "# Dense layer\n",
        "dense = Dense(units=128, activation='relu')(dropout)\n",
        "\n",
        "# Output layer\n",
        "output = Dense(units=20, activation='softmax')(dense)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUzd8GxnI-2i"
      },
      "outputs": [],
      "source": [
        "max_features =400000\n",
        "embedding_dim = 50\n",
        "sequence_length = 500\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_features, embedding_dim, input_length=sequence_length,\\\n",
        "                                    embeddings_regularizer = regularizers.l2(0.0005)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv1D(128,3, activation='relu',\\\n",
        "                                 kernel_regularizer = regularizers.l2(0.0005),\\\n",
        "                                 bias_regularizer = regularizers.l2(0.0005)))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(3, activation='sigmoid',\\\n",
        "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
        "                                bias_regularizer=regularizers.l2(0.001),))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), optimizer='Nadam', metrics=[\"CategoricalAccuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxwm1TsFc0BC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels\n",
        "encoded_labels = label_encoder.fit_transform(labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2TEo7HM0_B6",
        "outputId": "101475b1-c0b7-4330-87ae-f1314dee9c4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15963, 500)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK0OBxp51Pe2",
        "outputId": "1dd4ee97-6383-43a2-b51a-b5438b368829"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([15,  8,  8, ...,  9,  3, 19])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhpkIB_IPrFV",
        "outputId": "72beead4-ea90-4f36-b900-b53c28c91012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "449/449 [==============================] - 258s 575ms/step - loss: 1.7187 - accuracy: 0.4545 - val_loss: 2.7987 - val_accuracy: 0.2079\n",
            "Epoch 2/30\n",
            "449/449 [==============================] - 260s 580ms/step - loss: 1.5233 - accuracy: 0.5113 - val_loss: 2.8725 - val_accuracy: 0.2154\n",
            "Epoch 3/30\n",
            "449/449 [==============================] - 240s 536ms/step - loss: 1.3623 - accuracy: 0.5615 - val_loss: 3.0187 - val_accuracy: 0.2079\n",
            "Epoch 4/30\n",
            "449/449 [==============================] - 241s 536ms/step - loss: 1.2325 - accuracy: 0.5990 - val_loss: 3.3476 - val_accuracy: 0.1966\n",
            "Epoch 5/30\n",
            "449/449 [==============================] - 257s 572ms/step - loss: 1.1337 - accuracy: 0.6325 - val_loss: 3.5455 - val_accuracy: 0.2085\n",
            "Epoch 6/30\n",
            "449/449 [==============================] - 242s 538ms/step - loss: 1.0439 - accuracy: 0.6568 - val_loss: 3.5162 - val_accuracy: 0.2079\n",
            "Epoch 7/30\n",
            "449/449 [==============================] - 260s 579ms/step - loss: 0.9644 - accuracy: 0.6839 - val_loss: 3.7818 - val_accuracy: 0.2048\n",
            "Epoch 8/30\n",
            "449/449 [==============================] - 244s 542ms/step - loss: 0.9021 - accuracy: 0.7030 - val_loss: 3.8834 - val_accuracy: 0.1997\n",
            "Epoch 9/30\n",
            "449/449 [==============================] - 243s 541ms/step - loss: 0.8592 - accuracy: 0.7135 - val_loss: 4.0593 - val_accuracy: 0.1904\n",
            "Epoch 10/30\n",
            "449/449 [==============================] - 258s 576ms/step - loss: 0.8092 - accuracy: 0.7332 - val_loss: 4.2312 - val_accuracy: 0.1941\n",
            "Epoch 11/30\n",
            "449/449 [==============================] - 243s 542ms/step - loss: 0.7848 - accuracy: 0.7404 - val_loss: 4.2158 - val_accuracy: 0.1885\n",
            "Epoch 12/30\n",
            "449/449 [==============================] - 258s 573ms/step - loss: 0.7432 - accuracy: 0.7568 - val_loss: 4.4265 - val_accuracy: 0.1960\n",
            "Epoch 13/30\n",
            "449/449 [==============================] - 242s 539ms/step - loss: 0.7214 - accuracy: 0.7599 - val_loss: 4.3958 - val_accuracy: 0.1947\n",
            "Epoch 14/30\n",
            "449/449 [==============================] - 244s 543ms/step - loss: 0.6782 - accuracy: 0.7724 - val_loss: 4.7555 - val_accuracy: 0.1847\n",
            "Epoch 15/30\n",
            "176/449 [==========>...................] - ETA: 2:54 - loss: 0.5882 - accuracy: 0.7990"
          ]
        }
      ],
      "source": [
        "history = model.fit(texts_train, encoded_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2lzOww1YPs_"
      },
      "outputs": [],
      "source": [
        "# prompt: save model at current dir\n",
        "model.save('cnn_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q4AmkFKzLDp",
        "outputId": "379f91e7-24ae-4d4a-b872-90f1dae37548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/text_classification_with_CNN\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/text_classification_with_CNN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQhWCXL0yf9Z"
      },
      "outputs": [],
      "source": [
        "# prompt: load saved model\n",
        "model = tf.keras.models.load_model('cnn_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8495IXfHzRIQ",
        "outputId": "f8777e8b-2e91-457b-9bec-6195af174c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "499/499 [==============================] - 42s 82ms/step - loss: 1.3042 - accuracy: 0.7526\n",
            "Train accuracy: 0.7526154518127441\n"
          ]
        }
      ],
      "source": [
        "# prompt: evaluate on the train data\n",
        "train_results = model.evaluate(embedding_matrix_train, encoded_labels, batch_size=32)\n",
        "print(\"Train accuracy:\", train_results[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coLiM737i5bC",
        "outputId": "ac33f3aa-ed26-40bc-d6c5-aecfdc411633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 10s 81ms/step - loss: 1.5918 - accuracy: 0.6545\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.5918047428131104, 0.6544725894927979]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: evaluate above model on test data?\n",
        "encoded_labels_test = label_encoder.transform(labels_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "model.evaluate(embedding_matrix_test, encoded_labels_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RENXKnfmyWd-"
      },
      "source": [
        "**Observation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s96IyH0FybDN"
      },
      "source": [
        "65% test accuracy and 75% train accuracy. It can be improved by doing better featurization but skipping since our goal was just experiment. And main goal is to implement the char lavel cnn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLZbH-E_xXGZ"
      },
      "source": [
        "**The end**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}